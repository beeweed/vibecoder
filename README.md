# VibeCoder ğŸš€

An open-source AI-powered code generation platform with an **agent-based multi-call architecture**. Describe what you want to build, and the AI agent will think, plan, and execute step-by-step until your application is complete.

Built with â¤ï¸ by [beeweed](https://github.com/beeweed) and [ii-agent](https://github.com/anthropics/ii-agent)

![VibeCoder Screenshot](https://img.shields.io/badge/Next.js-15-black?style=flat-square&logo=next.js) ![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue?style=flat-square&logo=typescript) ![Tailwind CSS](https://img.shields.io/badge/Tailwind-3.4-38B2AC?style=flat-square&logo=tailwind-css)

## âœ¨ Features

### ğŸ¤– Agent-Based Multi-Call Architecture

Unlike traditional single-call LLM applications, VibeCoder uses a sophisticated **LLM-in-the-loop** architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  THINKING   â”‚ â”€â”€â–¶ â”‚  PLANNING   â”‚ â”€â”€â–¶ â”‚  STEP EXECUTION     â”‚ â”€â”€â–¶ â”‚ COMPLETION  â”‚
â”‚  (LLM #1)   â”‚     â”‚  (LLM #2)   â”‚     â”‚  (LLM #3, #4, ...)  â”‚     â”‚  (Final)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â–²       â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”˜
                                            Loop until done
```

1. **Thinking Phase** - Agent analyzes your request and understands the requirements
2. **Planning Phase** - Agent creates a detailed execution plan with actionable steps
3. **Step Execution** - Each step gets its own LLM call for focused, quality code generation
4. **Completion** - Agent summarizes what was built

### ğŸ¯ Key Capabilities

- **13 LLM Providers Supported** - OpenRouter, Anthropic, OpenAI, Gemini, Groq, DeepSeek, Mistral, Cohere, Fireworks, Cerebras, Hugging Face, Z.ai, and Chutes
- **Virtual File System** - All files managed in-browser with full CRUD operations
- **Monaco Editor** - Professional code editing with syntax highlighting and IntelliSense
- **Real-time Streaming** - Watch the AI think, plan, and code in real-time
- **Tool System** - AI can read existing files before modifying them
- **Export to ZIP** - Download your generated project as a ZIP file
- **Mobile Responsive** - Works on desktop and mobile devices

## ğŸš€ Quick Start

### Prerequisites

- Node.js 18+ or Bun
- An API key from any supported provider

### Installation

```bash
# Clone the repository
git clone https://github.com/beeweed/vibecoder.git
cd vibecoder

# Install dependencies
npm install
# or
bun install

# Start the development server
npm run dev
# or
bun dev
```

Open [http://localhost:3000](http://localhost:3000) in your browser.

### Configuration

1. Click the âš™ï¸ settings icon in the top-right corner
2. Select your preferred LLM provider
3. Enter your API key
4. (Optional) Adjust temperature and max tokens
5. Start building!

## ğŸ—ï¸ Architecture

```
src/
â”œâ”€â”€ app/                          # Next.js App Router
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ agent/               # Agent-based API routes
â”‚   â”‚   â”‚   â”œâ”€â”€ plan/            # Planning phase endpoint
â”‚   â”‚   â”‚   â”œâ”€â”€ execute-step/    # Step execution endpoint
â”‚   â”‚   â”‚   â””â”€â”€ complete/        # Completion summary endpoint
â”‚   â”‚   â”œâ”€â”€ chat/                # Legacy chat endpoint
â”‚   â”‚   â”œâ”€â”€ think/               # Thinking phase endpoint
â”‚   â”‚   â”œâ”€â”€ models/              # Model listing per provider
â”‚   â”‚   â””â”€â”€ tools/               # Tool execution (read_file)
â”‚   â”œâ”€â”€ page.tsx                 # Main 3-panel layout
â”‚   â””â”€â”€ layout.tsx               # Root layout
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ai-panel/
â”‚   â”‚   â”œâ”€â”€ AIPanel.tsx          # Main chat interface + agent loop
â”‚   â”‚   â”œâ”€â”€ AgentPlanDisplay.tsx # Plan visualization component
â”‚   â”‚   â””â”€â”€ ToolCallIndicator.tsx
â”‚   â”œâ”€â”€ editor/                  # Monaco editor integration
â”‚   â”œâ”€â”€ file-explorer/           # Virtual file tree
â”‚   â”œâ”€â”€ layout/                  # Header, navigation
â”‚   â””â”€â”€ ui/                      # shadcn/ui components
â”œâ”€â”€ stores/                      # Zustand state management
â”‚   â”œâ”€â”€ agentLoopStore.ts        # Agent phase state
â”‚   â”œâ”€â”€ chatStore.ts             # Messages + plan steps
â”‚   â”œâ”€â”€ editorStore.ts           # Editor tabs
â”‚   â”œâ”€â”€ fileSystemStore.ts       # Virtual file system
â”‚   â””â”€â”€ settingsStore.ts         # API keys (persisted)
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ systemPrompt.ts          # All agent phase prompts
â”‚   â”œâ”€â”€ parser.ts                # Streaming response parser
â”‚   â””â”€â”€ utils.ts                 # Utility functions
â””â”€â”€ types/
    â”œâ”€â”€ agentLoop.ts             # Agent phase types
    â”œâ”€â”€ chat.ts                  # Message types
    â””â”€â”€ files.ts                 # File system types
```

## ğŸ”§ Tech Stack

| Category | Technology |
|----------|------------|
| Framework | Next.js 15 (App Router, Turbopack) |
| Language | TypeScript |
| Styling | Tailwind CSS |
| UI Components | shadcn/ui, Radix UI |
| State Management | Zustand |
| Code Editor | Monaco Editor |
| Animations | Framer Motion |
| Icons | Lucide React |
| Linting | Biome |
| Package Manager | Bun / npm |

## ğŸŒ Supported LLM Providers

| Provider | Default Model | API Endpoint |
|----------|---------------|--------------|
| OpenRouter | claude-sonnet-4 | openrouter.ai |
| Anthropic | claude-sonnet-4 | api.anthropic.com |
| OpenAI | gpt-4o | api.openai.com |
| Google Gemini | gemini-2.5-flash | generativelanguage.googleapis.com |
| Groq | llama-3.3-70b | api.groq.com |
| DeepSeek | deepseek-chat | api.deepseek.com |
| Mistral | mistral-large-latest | api.mistral.ai |
| Cohere | command-a-03-2025 | api.cohere.com |
| Fireworks | llama-v3p1-70b | api.fireworks.ai |
| Cerebras | llama-3.3-70b | api.cerebras.ai |
| Hugging Face | Qwen3-Coder-480B | router.huggingface.co |
| Z.ai | glm-4.7 | api.z.ai |
| Chutes | Mistral-Small-3.1 | api.chutes.ai |

## ğŸ“– How the Agent Works

### Phase 1: Thinking
```
User: "Build me a todo app with React"

Agent Thinking: "The user wants to create a React todo application 
with add, complete, and delete functionality."
```

### Phase 2: Planning
```json
{
  "goal": "Build a React todo application",
  "steps": [
    { "title": "Create type definitions", "description": "Define Todo interface..." },
    { "title": "Build TodoItem component", "description": "Create individual todo..." },
    { "title": "Build TodoList component", "description": "Create the list container..." },
    { "title": "Implement main page", "description": "Integrate all components..." }
  ]
}
```

### Phase 3: Step Execution
Each step gets its own dedicated LLM call:
- Step 1 â†’ LLM creates `src/types/todo.ts`
- Step 2 â†’ LLM creates `src/components/TodoItem.tsx`
- Step 3 â†’ LLM creates `src/components/TodoList.tsx`
- Step 4 â†’ LLM creates `src/app/page.tsx`

### Phase 4: Completion
```
"I've built a complete todo application with add, complete, and delete 
functionality. The app uses React state management with a clean, modern 
design. You can start adding todos right away!"
```

## ğŸ› ï¸ Available Scripts

```bash
# Development
npm run dev          # Start dev server with Turbopack

# Build
npm run build        # Production build
npm run start        # Start production server

# Code Quality
npm run lint         # Run Biome linter
npm run format       # Format code with Biome
```

## ğŸ—ºï¸ Roadmap

- [x] Agent-based multi-call architecture
- [x] Planning before application creation
- [x] Step-by-step execution with individual LLM calls
- [ ] E2B sandbox for live code preview
- [ ] Web search integration
- [ ] Git operations (push, clone)
- [ ] Deploy to Vercel/Netlify
- [ ] Sub-agents architecture
- [ ] File upload with prompts
- [ ] Default file-system templates (Next.js, Python, etc.)
- [ ] Image file content fetching

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

## â­ Star History

If you find VibeCoder useful, please consider giving it a star! â­

---

<p align="center">
  Made with â¤ï¸ by the VibeCoder community
</p> 
